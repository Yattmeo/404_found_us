{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73961ab",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA): Financial Transactions Dataset\n",
    "\n",
    "**Dataset:** HuggingFace Financial_Transactions  \n",
    "**Focus:** MCC 5411 - Grocery Stores and Supermarkets  \n",
    "**Objective:** Analyze transaction patterns to prepare data for predictive modeling\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Sections:\n",
    "1. Data Loading and Initial Inspection\n",
    "2. Data Cleaning and Quality Checks\n",
    "3. Weekly Aggregation and Descriptive Statistics\n",
    "4. Feature Engineering (91 features)\n",
    "5. Final Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f9b86",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub as hf_hub\n",
    "\n",
    "hf_hub.login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Financial Transactions dataset from HuggingFace\n",
    "ds = load_dataset(\"thiru1711/Financial_Transactions\")\n",
    "\n",
    "# Display sample record\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = ds['train'].to_pandas()\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20709977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for analysis\n",
    "df_working = df[['transaction_id','date','card_brand','merchant_id', 'mcc','mcc_description', 'amount']]\n",
    "\n",
    "# Display dataset info\n",
    "df_working.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff060a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze MCC distribution to identify top categories\n",
    "mcc_counts = df_working['mcc'].value_counts()\n",
    "print(\"Top 10 Most Frequent MCCs:\")\n",
    "print(mcc_counts.head(10))\n",
    "\n",
    "# Include MCC descriptions\n",
    "print(\"\\n\\nTop 10 MCCs with Descriptions:\")\n",
    "top_mccs = df_working.groupby(['mcc', 'mcc_description']).size().reset_index(name='count')\n",
    "top_mccs = top_mccs.sort_values('count', ascending=False).head(10)\n",
    "top_mccs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0322c2",
   "metadata": {},
   "source": [
    "## 2. Focus on MCC 5411: Grocery Stores and Supermarkets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b25d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for MCC 5411 only (Grocery Stores, Supermarkets)\n",
    "gs_df = df_working[df_working['mcc'] == '5411'].copy()\n",
    "\n",
    "# Convert date column to datetime\n",
    "gs_df['date'] = pd.to_datetime(gs_df['date'])\n",
    "\n",
    "gs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6338b4",
   "metadata": {},
   "source": [
    "## 3. Weekly Aggregation and Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a week column for grouping\n",
    "gs_df['week'] = gs_df['date'].dt.to_period('W')\n",
    "\n",
    "# Group by week and calculate metrics\n",
    "weekly_stats = gs_df.groupby('week').agg({\n",
    "    'transaction_id': 'count',  # Count of transactions\n",
    "    'amount': ['sum', 'mean']   # Total and average amount\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "weekly_stats.columns = ['transaction_count', 'total_amount', 'avg_amount']\n",
    "weekly_stats.index = weekly_stats.index.to_timestamp()\n",
    "\n",
    "# Round numeric columns\n",
    "weekly_stats['total_amount'] = weekly_stats['total_amount'].round(2)\n",
    "weekly_stats['avg_amount'] = weekly_stats['avg_amount'].round(2)\n",
    "\n",
    "print(f\"Weekly dataset created: {len(weekly_stats)} weeks\")\n",
    "print(f\"Date range: {weekly_stats.index.min()} to {weekly_stats.index.max()}\")\n",
    "weekly_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3,1, figsize=(15,10))\n",
    "\n",
    "# Plot transaction count\n",
    "axes[0].plot(weekly_stats.index, weekly_stats['transaction_count'], linewidth=1.5)\n",
    "axes[0].set_ylabel('Total Transactions', fontsize=11)\n",
    "axes[0].set_title('Transaction Count per Week', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot total amount\n",
    "axes[1].plot(weekly_stats.index, weekly_stats['total_amount'], linewidth=1.5, color='green')\n",
    "axes[1].set_ylabel('Total Amount ($)', fontsize=11)\n",
    "axes[1].set_title('Total Amount per Week', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot average amount\n",
    "axes[2].plot(weekly_stats.index, weekly_stats['avg_amount'], linewidth=1.5, color='orange')\n",
    "axes[2].set_ylabel('Average Amount ($)', fontsize=11)\n",
    "axes[2].set_xlabel('Week', fontsize=11)\n",
    "axes[2].set_title('Average Transaction Amount per Week', fontsize=12)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('MCC 5411: Grocery Stores & Supermarkets - Weekly Metrics', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive descriptive statistics\n",
    "weekly_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed statistical analysis\n",
    "print(\"=\"*80)\n",
    "print(\"WEEKLY STATISTICS - Detailed Analysis\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset Size: {len(weekly_stats)} weeks\")\n",
    "print(f\"Date Range: {weekly_stats.index.min().strftime('%Y-%m-%d')} to {weekly_stats.index.max().strftime('%Y-%m-%d')}\\n\")\n",
    "\n",
    "for col in ['transaction_count', 'total_amount', 'avg_amount']:\n",
    "    print(f\"\\n{col.upper().replace('_', ' ')}:\")\n",
    "    mean_val = weekly_stats[col].mean()\n",
    "    std_val = weekly_stats[col].std()\n",
    "    min_val = weekly_stats[col].min()\n",
    "    max_val = weekly_stats[col].max()\n",
    "    cv = (std_val / mean_val) * 100  # Coefficient of variation (%)\n",
    "    \n",
    "    print(f\"  Mean: {mean_val:,.2f}\")\n",
    "    print(f\"  Std Dev: {std_val:,.2f}\")\n",
    "    print(f\"  CV (Coefficient of Variation): {cv:.2f}%\")\n",
    "    print(f\"  Range: {min_val:,.2f} to {max_val:,.2f}\")\n",
    "\n",
    "# Growth metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GROWTH METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for col in ['transaction_count', 'total_amount', 'avg_amount']:\n",
    "    first_val = weekly_stats[col].iloc[0]\n",
    "    last_val = weekly_stats[col].iloc[-1]\n",
    "    total_growth = ((last_val - first_val) / first_val) * 100\n",
    "    weekly_avg_growth = total_growth / len(weekly_stats)\n",
    "    \n",
    "    print(f\"\\n{col.upper().replace('_', ' ')}:\")\n",
    "    print(f\"  First value: {first_val:,.2f}\")\n",
    "    print(f\"  Last value: {last_val:,.2f}\")\n",
    "    print(f\"  Total growth: {total_growth:.2f}%\")\n",
    "    print(f\"  Average weekly growth: {weekly_avg_growth:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8c825",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks: Identifying Incomplete Weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc84373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for incomplete weeks (first and last weeks often incomplete)\n",
    "mean_count = weekly_stats['transaction_count'].mean()\n",
    "first_count = weekly_stats['transaction_count'].iloc[0]\n",
    "last_count = weekly_stats['transaction_count'].iloc[-1]\n",
    "\n",
    "# Set threshold at 70% of mean weekly transactions\n",
    "threshold = 0.7 * mean_count\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY CHECK: Identifying Incomplete Weeks\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nMean weekly transaction count: {mean_count:,.0f}\")\n",
    "print(f\"Threshold (70% of mean): {threshold:,.0f}\\n\")\n",
    "\n",
    "print(\"FIRST WEEK:\")\n",
    "print(f\"  Week: {weekly_stats.index[0].strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Transaction count: {first_count:,.0f}\")\n",
    "print(f\"  Percentage of mean: {(first_count/mean_count)*100:.1f}%\")\n",
    "remove_first = first_count < threshold\n",
    "print(f\"  Assessment: {'âŒ INCOMPLETE - REMOVE' if remove_first else 'âœ… Complete'}\")\n",
    "\n",
    "print(\"\\nLAST WEEK:\")\n",
    "print(f\"  Week: {weekly_stats.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Transaction count: {last_count:,.0f}\")\n",
    "print(f\"  Percentage of mean: {(last_count/mean_count)*100:.1f}%\")\n",
    "remove_last = last_count < threshold\n",
    "print(f\"  Assessment: {'âŒ INCOMPLETE - REMOVE' if remove_last else 'âœ… Complete'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove incomplete weeks if necessary\n",
    "weekly_clean = weekly_stats.copy()\n",
    "\n",
    "if remove_first:\n",
    "    weekly_clean = weekly_clean.iloc[1:]\n",
    "    print(f\"âœ… Removed first week: {weekly_stats.index[0].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "if remove_last:\n",
    "    weekly_clean = weekly_clean.iloc[:-1]\n",
    "    print(f\"âœ… Removed last week: {weekly_stats.index[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEANED DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original weeks: {len(weekly_stats)}\")\n",
    "print(f\"Cleaned weeks: {len(weekly_clean)}\")\n",
    "print(f\"Weeks removed: {len(weekly_stats) - len(weekly_clean)}\")\n",
    "print(f\"\\nNew date range: {weekly_clean.index.min().strftime('%Y-%m-%d')} to {weekly_clean.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "weekly_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be588cf",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering: Creating 91 Predictive Features\n",
    "\n",
    "We'll create comprehensive features to capture temporal patterns, trends, and seasonality without using external data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature-engineered dataset\n",
    "features_df = weekly_clean.copy()\n",
    "\n",
    "# 1. TEMPORAL FEATURES\n",
    "features_df['week_of_year'] = features_df.index.isocalendar().week\n",
    "features_df['month'] = features_df.index.month\n",
    "features_df['quarter'] = features_df.index.quarter\n",
    "features_df['year'] = features_df.index.year\n",
    "features_df['day_of_year'] = features_df.index.dayofyear\n",
    "\n",
    "# 2. CYCLICAL ENCODING (sine/cosine to preserve cyclical nature)\n",
    "features_df['week_sin'] = np.sin(2 * np.pi * features_df['week_of_year'] / 52)\n",
    "features_df['week_cos'] = np.cos(2 * np.pi * features_df['week_of_year'] / 52)\n",
    "features_df['month_sin'] = np.sin(2 * np.pi * features_df['month'] / 12)\n",
    "features_df['month_cos'] = np.cos(2 * np.pi * features_df['month'] / 12)\n",
    "\n",
    "# 3. TREND FEATURE\n",
    "features_df['time_index'] = np.arange(len(features_df))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEMPORAL FEATURES ADDED\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… Basic temporal: week_of_year, month, quarter, year, day_of_year\")\n",
    "print(\"âœ… Cyclical encoding: week_sin/cos, month_sin/cos (preserves seasonality)\")\n",
    "print(\"âœ… Trend: time_index (linear progression)\")\n",
    "print(f\"\\nTotal temporal features: 10\")\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12659e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LAG FEATURES (previous week's values)\n",
    "lag_periods = [1, 2, 3, 4, 8, 12, 52]  # 1w, 2w, 3w, 1m, 2m, 3m, 1y\n",
    "\n",
    "for lag in lag_periods:\n",
    "    for col in ['transaction_count', 'total_amount', 'avg_amount']:\n",
    "        features_df[f'{col}_lag{lag}'] = features_df[col].shift(lag)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LAG FEATURES ADDED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Lag periods: {lag_periods} weeks\")\n",
    "print(f\"For metrics: transaction_count, total_amount, avg_amount\")\n",
    "print(f\"\\nTotal lag features: {len(lag_periods) * 3} = 21\")\n",
    "print(f\"\\nExamples:\")\n",
    "print(f\"  - transaction_count_lag1 = transaction count from 1 week ago\")\n",
    "print(f\"  - transaction_count_lag52 = transaction count from same week last year\")\n",
    "\n",
    "# Show sample lag features\n",
    "features_df.iloc[52:55][['transaction_count', 'transaction_count_lag1', 'transaction_count_lag4', 'transaction_count_lag52']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ROLLING WINDOW FEATURES (moving averages and statistics)\n",
    "rolling_windows = [4, 8, 12, 26]  # 1 month, 2 months, 3 months, 6 months\n",
    "\n",
    "for window in rolling_windows:\n",
    "    for col in ['transaction_count', 'total_amount', 'avg_amount']:\n",
    "        # Rolling statistics using shift(1) to prevent data leakage\n",
    "        features_df[f'{col}_ma{window}'] = features_df[col].shift(1).rolling(window=window).mean()\n",
    "        features_df[f'{col}_std{window}'] = features_df[col].shift(1).rolling(window=window).std()\n",
    "        features_df[f'{col}_min{window}'] = features_df[col].shift(1).rolling(window=window).min()\n",
    "        features_df[f'{col}_max{window}'] = features_df[col].shift(1).rolling(window=window).max()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ROLLING WINDOW FEATURES ADDED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Windows: {rolling_windows} weeks\")\n",
    "print(f\"Statistics: Moving Average, Std Dev, Min, Max\")\n",
    "print(f\"For metrics: transaction_count, total_amount, avg_amount\")\n",
    "print(f\"\\nTotal rolling features: {len(rolling_windows) * 3 * 4} = 48\")\n",
    "print(f\"\\nâš ï¸  All rolling features use shift(1) to prevent data leakage\")\n",
    "print(f\"   (only past data used, not current week)\")\n",
    "\n",
    "# Show sample rolling features\n",
    "features_df.iloc[50:55][['transaction_count', 'transaction_count_ma4', 'transaction_count_std4', 'transaction_count_ma26']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. DIFFERENCE FEATURES (changes and growth rates)\n",
    "for col in ['transaction_count', 'total_amount', 'avg_amount']:\n",
    "    # Week-over-week change\n",
    "    features_df[f'{col}_diff1'] = features_df[col].diff(1)\n",
    "    features_df[f'{col}_pct_change1'] = features_df[col].pct_change(1)\n",
    "    \n",
    "    # Year-over-year change (52 weeks)\n",
    "    features_df[f'{col}_diff52'] = features_df[col].diff(52)\n",
    "    features_df[f'{col}_pct_change52'] = features_df[col].pct_change(52)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIFFERENCE FEATURES ADDED\")\n",
    "print(\"=\"*80)\n",
    "print(\"For metrics: transaction_count, total_amount, avg_amount\")\n",
    "print(\"\\nFeatures created:\")\n",
    "print(\"  - diff1: Week-over-week absolute change\")\n",
    "print(\"  - pct_change1: Week-over-week % change\")\n",
    "print(\"  - diff52: Year-over-year absolute change\")\n",
    "print(\"  - pct_change52: Year-over-year % change\")\n",
    "print(f\"\\nTotal difference features: {3 * 4} = 12\")\n",
    "\n",
    "# Show sample difference features\n",
    "features_df.iloc[52:57][['transaction_count', 'transaction_count_diff1', 'transaction_count_pct_change1', 'transaction_count_diff52']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb33a5",
   "metadata": {},
   "source": [
    "## 6. Feature Summary and Final Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ac600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete feature summary\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Original Metrics: 3\")\n",
    "print(f\"   - transaction_count, total_amount, avg_amount\")\n",
    "\n",
    "print(f\"\\nðŸ“… Temporal Features: 10\")\n",
    "print(f\"   - Basic: week_of_year, month, quarter, year, day_of_year\")\n",
    "print(f\"   - Cyclical: week_sin, week_cos, month_sin, month_cos\")\n",
    "print(f\"   - Trend: time_index\")\n",
    "\n",
    "print(f\"\\nâ®ï¸  Lag Features: 21\")\n",
    "print(f\"   - Lags: {lag_periods}\")\n",
    "print(f\"   - For each of 3 metrics\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Rolling Features: 48\")\n",
    "print(f\"   - Windows: {rolling_windows}\")\n",
    "print(f\"   - Stats: mean, std, min, max\")\n",
    "print(f\"   - For each of 3 metrics\")\n",
    "\n",
    "print(f\"\\nðŸ”„ Difference Features: 12\")\n",
    "print(f\"   - Absolute and % change\")\n",
    "print(f\"   - 1-week and 52-week differences\")\n",
    "print(f\"   - For each of 3 metrics\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOTAL FEATURES IN DATASET: {features_df.shape[1]}\")\n",
    "print(f\"  - 3 target metrics\")\n",
    "print(f\"  - {features_df.shape[1] - 3} predictor features\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Dataset Status:\")\n",
    "print(f\"   Total rows: {len(features_df)}\")\n",
    "print(f\"   Rows with NaN: {features_df.isnull().any(axis=1).sum()}\")\n",
    "print(f\"   Complete cases: {features_df.dropna().shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values (due to lags and rolling windows)\n",
    "features_clean = features_df.dropna().copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL CLEAN DATASET FOR MODELING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Cleaned dataset: {len(features_clean)} weeks\")\n",
    "print(f\"   Removed {len(features_df) - len(features_clean)} weeks with NaN values\")\n",
    "print(f\"\\nðŸ“… Date range:\")\n",
    "print(f\"   Start: {features_clean.index.min().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   End: {features_clean.index.max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Duration: ~{len(features_clean) / 52:.1f} years\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset dimensions:\")\n",
    "print(f\"   Rows (weeks): {features_clean.shape[0]}\")\n",
    "print(f\"   Columns (features): {features_clean.shape[1]}\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset ready for modeling!\")\n",
    "\n",
    "features_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c042e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset for modeling\n",
    "features_clean.to_csv('grocery_store_features_clean.csv')\n",
    "print(\"âœ… Dataset saved to: grocery_store_features_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c166abb",
   "metadata": {},
   "source": [
    "## 7. Quick Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Select key features for correlation analysis (transaction_count example)\n",
    "key_features = [\n",
    "    'transaction_count',\n",
    "    'transaction_count_lag1', 'transaction_count_lag4', 'transaction_count_lag52',\n",
    "    'transaction_count_ma4', 'transaction_count_ma12',\n",
    "    'transaction_count_diff1', 'transaction_count_pct_change52',\n",
    "    'week_of_year', 'month', 'time_index',\n",
    "    'week_sin', 'week_cos'\n",
    "]\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = features_clean[key_features].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix: Key Features vs Transaction Count', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strongest correlations\n",
    "print(\"=\"*80)\n",
    "print(\"STRONGEST CORRELATIONS WITH TRANSACTION_COUNT\")\n",
    "print(\"=\"*80)\n",
    "corr_with_target = corr_matrix['transaction_count'].abs().sort_values(ascending=False)\n",
    "print(corr_with_target.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035076bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… EDA Complete!\n",
    "\n",
    "**Summary:**\n",
    "- âœ… Loaded Financial Transactions dataset from HuggingFace\n",
    "- âœ… Filtered for MCC 5411 (Grocery Stores)\n",
    "- âœ… Created weekly aggregations (512 weeks â†’ 512 clean weeks)\n",
    "- âœ… Identified and removed incomplete weeks\n",
    "- âœ… Engineered 91 features (temporal, lag, rolling, difference)\n",
    "- âœ… Final clean dataset: **460 weeks** with **94 columns**\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to **02_Preliminary_Modelling.ipynb** for model training and evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
